<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="https://mathklim.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://mathklim.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2022-01-03T22:20:54+00:00</updated><id>https://mathklim.github.io/feed.xml</id><title type="html">Mathieu Klimczak</title><subtitle>Mathieu&apos;s Website.
</subtitle><entry><title type="html">Gradient Centralization (FR)</title><link href="https://mathklim.github.io/blog/2021/gradient-centralization/" rel="alternate" type="text/html" title="Gradient Centralization (FR)" /><published>2021-11-29T00:00:00+00:00</published><updated>2021-11-29T00:00:00+00:00</updated><id>https://mathklim.github.io/blog/2021/gradient-centralization</id><content type="html" xml:base="https://mathklim.github.io/blog/2021/gradient-centralization/"><![CDATA[<h2 id="abstract">Abstract</h2>

<p>Optimization techniques are of great importance to effectively and efficiently train a deep neural network (DNN). It has been shown that using the first and second order statistics (e.g., mean and variance) to perform Z-score standardization on network activations or weight vectors, such as batch normalization (BN) and weight standardization (WS), can improve the training performance. Different from these existing methods that mostly operate on activations or weights, we present a new optimization technique, namely gradient centralization (GC), which operates directly on gradients by centralizing the gradient vectors to have zero mean. GC can be viewed as a projected gradient descent method with a constrained loss function. We show that GC can regularize both the weight space and output feature space so that it can boost the generalization performance of DNNs. Moreover, GC improves the Lipschitzness of the loss function and its gradient so that the training process becomes more efficient and stable. GC is very simple to implement and can be easily embedded into existing gradient based DNN optimizers with only one line of code. It can also be directly used to fine-tune the pre-trained DNNs. Our experiments on various applications, including general image classification, fine-grained image classification, detection and segmentation, demonstrate that GC can consistently improve the performance of DNN learning. The code of GC can be found at <a href="https://github.com/Yonghongwei/Gradient-Centralization">this https URL</a>.</p>

<ul>
  <li><a href="https://arxiv.org/abs/2004.01461">ArXiv</a></li>
</ul>

<p>La centralisation du gradient, est une nouvelle méthode d’optimisation des réseaux de neurones (plutôt une modification de celles déjà existantes), qui se concentre sur le gradient lui même afin de le centrer, i.e. étant donné le vecteur gradient \(\overrightarrow{\mathrm{grad}}\) obtenu en différentiant la fonction de perte \(\mathcal{L}_{\vartheta}\), on le centralise de façon à obtenir \(\overrightarrow{\mathrm{grad}}_{CG}\) vérifiant l’églite suivante.</p>

\[\mathbb{E}(\overrightarrow{\mathrm{grad}}_{CG}) =0\]

<p>On le verra par la suite, cette méthode peut être vu de façon géométrique comme une projection orthogonale du gradient sur un hyperplan contraignant la fonction de perte \(\mathcal{L}_{\vartheta}\).</p>

<h2 id="introduction">Introduction</h2>

<p>L’éfficacité de l’entraînement des réseaux de neurones dépend beaucoup des techniques d’optimization employées.</p>

<p>Le choix d’un optimiseur (Adam, RMSProp, Nesterov, etc) se fait généralement sur deux critères:</p>

<ol>
  <li>L’accélération de l’entraînement qui en résulte.</li>
  <li>L’amélioration de la capacité de généralisation du réseau.</li>
</ol>

<p>D’autres techniques d’optimisation existent :</p>
<ul>
  <li>Les méthodes d’initialisation des poids (He, Xavier etc),</li>
  <li>Le choix des fonctions d’activations (ReLU, mish),</li>
  <li>Le clipping du gradient,</li>
  <li>Mettre en place un taux d’apprentissage adaptatif.</li>
</ul>

<p>Il existe aussi des techniques de normalisation et de standardisation des poids, <strong>mais ces dernières ne sont pas applicables aux modèles pré-entrainés, dont les poids ne peuvent pas être centrés, normés</strong> (sinon on perd l’intérêt du pré-entraînement).</p>

<p>L’article apporte deux contributions :</p>

<ul>
  <li>Une nouvelle technique d’optimisation basée sur la centralisation du gradient, qui est facilement implémentable, <a href="https://keras.io/examples/vision/gradient_centralization/#implement-gradient-centralization">voir le tutoriel sur keras.io</a>.</li>
  <li>Les aspects théoriques de cette centralisation du gradient. L’article montre en particuliers que la centralisation du gradient :
    <ol>
      <li>contraint la fonction de perte en ajoutant une nouvelle contrainte sur le vecteur de poids, ce qui permet de régulariser l’espace des poids ainsi de celui des features de sortie.</li>
      <li>Améliore les propriétés lipschitzienne de la fonction de perte.</li>
    </ol>
  </li>
</ul>

<h3 id="définition">Définition</h3>

<ol>
  <li>Une fonction \(f\) entre deux espaces métriques \(f \, : \, (X, d_{1}) \rightarrow (Y, d_{2})\) est lipschitz s’il existe une constante \(C &gt;0\) telle que :
\(\forall x, y \in X, d_{2}(f(x), f(y)) \leq C d_{1}(x,y).\)
En particulier, la fonction \(f\) est continue.</li>
  <li>Une fonction différentiable \(f\) entre deux espaces normés \(f \, : \, (X, d_{1}) \rightarrow (Y, d_{2})\) est lipschitz lisse s’il existe une constante \(C &gt;0\) telle que :
\(\forall x, y \in X, \| \nabla f(x) - \nabla f(y) \| \leq C \| x - y \|.\)
(\(C\)-Lipschitz continuous gradient)</li>
  <li>La constante \(C\) est appélée la constante de Lipschitz de \(f\).</li>
</ol>

<h3 id="infos">Infos</h3>

<ul>
  <li><a href="https://math.stackexchange.com/questions/673898/lipschitz-smoothness-strong-convexity-and-the-hessian">Lipschitz Smoothness, Strong Convexity and the Hessian</a></li>
  <li><a href="https://xingyuzhou.org/blog/notes/Lipschitz-gradient">Lipschitz continuous gradient</a></li>
</ul>

<hr />

<h2 id="centralisation-du-gradient">Centralisation du gradient</h2>

<p>On sait que la <em>Batch Normalization</em> permet de réduire la constante de Lipschitz de la fonction de perte \(\mathcal{L}_{\vartheta}\) (TODO : Sources) et rend les gradients <em>plus lipschitz continus</em>.</p>

<p>Cependant, si la <em>batch size</em> est petite, la <em>Group Normalization</em> est plus appropriée, on perd alors les propriétés citées ci-dessus.</p>

<p>Peut-on directement opérer sur le gradient pour stabiliser l’entraînement ? Normaliser le gradient n’améliore pas la stabilité de l’entraînement, on va dire le centrer, ie le rendre d’espérance nulle.</p>

<hr />

<h2 id="notations-et-conventions">Notations et conventions</h2>

<ul>
  <li>Lorsque l’on parlera qu’une couche dense, on notera sa matrice de poids</li>
</ul>

\[W_{fc} \in \mathbb{R}^{C_{in} \times C_{out}}.\]

<ul>
  <li>Lorsque l’on parlera d’une couche convolutive, on notera sa matrice de poids</li>
</ul>

\[W_{conv} \in \mathbb{R}^{C_{in} \times C_{out} \times (k_{1}k_{2})},\]

<p>\(k_{1}\) et \(k_{2}\) étant les dimensions des noyaux de convolutions.</p>

<ul>
  <li>
    <p>Pour une couche quelconque, on utilisera la notation unifiée \(W \in \mathbb{R}^{M \times N}\). Pour \(W_{fc}\), \(M = C_{in}\), et pour \(W_{fc}\), \(M = C_{in}k_{1}k_{2}\).</p>
  </li>
  <li>
    <p>Pour une couche \(W \in \mathbb{R}^{M \times N}\), on notera \(w_{i} \in \mathbb{R}^{M}, i = 1, \dots, N\) sa \(i\)-ième colonne.</p>
  </li>
</ul>

<h3 id="convention-matricielle">Convention matricielle</h3>

<p>pour un vecteur d’entrée \(X\) de la couche \(W\), les features en sortie de la couche sont alors données par la formule suivante.</p>

\[out_{W}(X) := W^{T} \cdot X \qquad W^{T} \in \mathbb{R}^{C_{out} \times C_{in}}\]

<p>Ce qui veut dire que par convention, les vecteurs considérés ici sont des <strong>vecteurs colonnes</strong>.</p>

\[out_{W}(X) =
\left[
\begin{array}{ccc}
    \rule[.5ex]{2.5ex}{0.5pt} &amp; w^{T}_{1} &amp; \rule[.5ex]{2.5ex}{0.5pt} \\
    \rule[.5ex]{2.5ex}{0.5pt} &amp; w^{T}_{2} &amp; \rule[.5ex]{2.5ex}{0.5pt}
\end{array}
\right]^{T}
\cdot
\begin{bmatrix}
        x_{1} \\
        x_{2}
\end{bmatrix}\]

\[W^{T} \cdot X =
\left[
\begin{array}{ccc}
    w_{1,1} &amp; w_{1,2} &amp; w_{1,3} \\
    w_{2,1} &amp; w_{2,2} &amp; w_{2,3}
\end{array}
\right]^{T}
\cdot
\begin{bmatrix}
        x_{1} \\
        x_{2}
\end{bmatrix}\]

<ul>
  <li>
    <p>Pour la fonction de perte \(\mathcal{L}_{\vartheta}\), on notera son gradient \(\nabla_{W}\mathcal{L}_{\vartheta}\) par rapport à la matrice de poids \(W\), et \(\nabla_{w_{i}}\mathcal{L}_{\vartheta}\) son gradient par rapport au vecteur de poids \(w_{i}\).</p>
  </li>
  <li>
    <p>On note \(\mathbf{e} := \frac{1}{\sqrt{M}}\cdot \mathbb{1}_{M}\), vecteur unitaire de \(\mathbb{R}^{M}\).</p>
  </li>
</ul>

<hr />

<h2 id="formulation-de-la-centralisation-du-gradient">Formulation de la centralisation du gradient</h2>

<p>Pour une couche dense ou convolutive, supposons que l’on a obtenu le gradient via rétropropagation.</p>

<p>Pour un vecteur de poids \(w_{i}\), de gradient \(\nabla_{w_{i}}\mathcal{L}_{\vartheta}\), l’opérateur de centralisation du gradient, noté \(\Phi_{CG}\), est alors défini comme suit.</p>

\[\Phi_{CG}(\nabla_{w_{i}}\mathcal{L}_{\vartheta}) := \nabla_{w_{i}}\mathcal{L}_{\vartheta} - \mathbb{E}(\nabla_{w_{i}}\mathcal{L}_{\vartheta})\]

<p>Où</p>

\[\mathbb{E}(\nabla_{w_{i}}\mathcal{L}_{\vartheta}) := \frac{1}{M}\sum_{j=1}^{M}\nabla_{w_{i,j}}\mathcal{L}_{\vartheta}.\]

<p>En d’autres termes, pour une matrice de poids, on calcule la moyenne de chaque vecteur colonne de cette matrice et on retire cette moyenne à ses colonnes.</p>

<hr />

<h2 id="formulation-matricielle-et-représentation-géométrique">Formulation matricielle et représentation géométrique</h2>

<hr />

<h3 id="définition--produit-de-kronecker">Définition : Produit de Kronecker</h3>

<p>Pour \(x\) et \(y\) deux vecteurs colonnes de dimensions \(M\), respectivement \(N\), le prodouit de Kronecker de \(x\) et \(y\), noté \(x \cdot y^{T}\) ou \(x \otimes y\) est alors défini de la façon suivante.</p>

\[x \cdot y^{T} := \left[ x_{i} \cdot y_{j}\right]\]

<p>Où \(\left[ x_{i} \cdot y_{j}\right] \in \mathbb{R}^{M \times N}\) est la matrice dont les coefficients en les coordonnées \((i,j)\) sont données par le produit \(x_{i} \cdot y_{j}\).</p>

<p>Ce produit n’est pas commutatif. C’est un cas particulier de produit tensoriel, <a href="https://fr.wikipedia.org/wiki/Produit_matriciel#Produit_de_Kronecker">voir ici</a>.</p>

<hr />

<p>Dans le cas qui nous intéresse ici, on applique le produit de Kronecker au vecteur suivant : \(\mathbf{e}\).</p>

\[\begin{align*}
\mathbf{e} \otimes \mathbf{e} &amp; := \frac{1}{\sqrt{M}}\cdot \mathbb{1} \otimes \frac{1}{\sqrt{M}}\cdot \mathbb{1} \\
                                &amp; = \frac{1}{M}\cdot \mathbb{1} \otimes \mathbb{1} \\
                                &amp; = \frac{1}{M}\cdot \mathbb{1} \otimes \mathbb{1} \\
                                &amp; = \frac{1}{M}\cdot\left[
                                    \begin{array}{ccc}
                                        1 &amp; \cdots &amp; 1 \\
                                        \vdots &amp;  &amp; \vdots \\
                                        1 &amp; \cdots &amp; 1
                                    \end{array}
                                    \right]
\end{align*}\]

<p>Ici, \(\mathbb{1} \otimes \mathbb{1}\) est donc une matrice carrée de taille \(M \times M\) où tous les coefficients sont égaux à \(1\).</p>

<p>Remarquons que dans l’autre sens, le produit scalaire \(\mathbf{e}^{T} \cdot \mathbf{e}\) est égal à \(1\), car \(\mathbf{e}\) est par définition un vecteur unitaire.</p>

<p>Pour une matrice de poids \(W\), de gradient \(\nabla_{W}\mathcal{L}_{\vartheta}\), l’opérateur de centralisation du gradient, noté \(\Phi_{CG}\), est alors défini comme suit.</p>

\[\Phi_{CG}(\nabla_{W}\mathcal{L}_{\vartheta}) := P(\nabla_{W}\mathcal{L}_{\vartheta})\]

<p>Où \(P := \mathbb{I}_{M} - \mathbf{e} \otimes \mathbf{e}\). \(\mathbb{I}_{M}\) étant la matrice identité de taille \(M\).</p>

<p>Avant de voir les propriétés de l’opérateur \(P\), vérifions que l’on obtient bien le même résultat avec cette définition que la précédente.</p>

<p>On a \(\nabla_{W}\mathcal{L}_{\vartheta} := \left[ \nabla_{w_{1}}\mathcal{L}_{\vartheta} , \cdots, \nabla_{w_{N}}\mathcal{L}_{\vartheta} \right]\), d’où</p>

\[\begin{align*}
    \Phi_{CG}(\nabla_{W}\mathcal{L}_{\vartheta}) &amp; = \left(\mathbb{I}_{M} - \mathbf{e} \otimes \mathbf{e} \right)\cdot \left[ \nabla_{w_{1}}\mathcal{L}_{\vartheta} , \cdots, \nabla_{w_{N}}\mathcal{L}_{\vartheta} \right] \\
                                                 &amp; = \left[ \nabla_{w_{1}}\mathcal{L}_{\vartheta} , \cdots, \nabla_{w_{N}}\mathcal{L}_{\vartheta} \right] - \frac{1}{M}\cdot \mathbb{1} \cdot \mathbb{1}^{T} \cdot \left[ \nabla_{w_{1}}\mathcal{L}_{\vartheta} , \cdots, \nabla_{w_{N}}\mathcal{L}_{\vartheta} \right].
    \end{align*}\]

<p>Par linéarité, il suffit de le vérifier sur chaque colonne.</p>

\[\begin{align*}
    \nabla_{w_{i}}\mathcal{L}_{\vartheta} - \frac{1}{M}\cdot \mathbb{1} \cdot \mathbb{1}^{T} \cdot \nabla_{w_{i}}\mathcal{L}_{\vartheta} &amp; = \nabla_{w_{i}}\mathcal{L}_{\vartheta} - \frac{1}{M}\cdot \mathbb{1} \cdot \left( \mathbb{1}^{T} \cdot \nabla_{w_{i}}\mathcal{L}_{\vartheta} \right) \\
    &amp; = \nabla_{w_{i}}\mathcal{L}_{\vartheta} - \frac{1}{M}\cdot \mathbb{1} \cdot \left(\sum_{j=1}^{M}\nabla_{w_{i,j}}\mathcal{L}_{\vartheta}\right) \\
    &amp; = \nabla_{w_{i}}\mathcal{L}_{\vartheta} - \mathbb{E}(\nabla_{w_{i}}\mathcal{L}_{\vartheta})
    \end{align*}\]

<p>On a donc bien le même résultat, peu importe la définition. Passons donc maintenant aux propriétées de l’opérateur \(P\).</p>

<h3 id="théorème">Théorème</h3>

<p>L’opérateur \(P\) est idempotent et définit une projection sur l’hyperplan orthogonal au vecteur unitaire \(\mathbf{e}^{T}\).</p>

<h4 id="preuve">Preuve</h4>

<p>Montrons premièrement que c’est un opérateur idempotent. Pour cela il suffit de montrer que \(P^{2} = P = P^{T}\).</p>

<p>Tout d’abord, on a \(P^{T} = (\mathbb{I}_{M} - \mathbf{e} \otimes \mathbf{e})^{T} = \mathbb{I}_{M}^{T} - (\mathbf{e} \cdot \mathbf{e}^{T})^{T} = \mathbb{I}_{M} - \mathbf{e} \cdot \mathbf{e}^{T} = P\).</p>

<p>D’où,</p>

\[\begin{align*}
        P^{2} = P^{T}P &amp; = (\mathbb{I}_{M} - \mathbf{e} \otimes \mathbf{e})^{T} \cdot (\mathbb{I}_{M} - \mathbf{e} \otimes \mathbf{e}) \\
                       &amp; = \mathbb{I}_{M} - 2 \mathbf{e}\cdot \mathbf{e}^{T} + \mathbf{e}\cdot (\mathbf{e}^{T}\cdot \mathbf{e})\cdot \mathbf{e}^{T} \\
                       &amp; = \mathbb{I}_{M} - 2 \mathbf{e}\cdot \mathbf{e}^{T} + \mathbf{e}\cdot\mathbf{e}^{T} \\
                       &amp; = \mathbb{I}_{M} - \mathbf{e}\cdot \mathbf{e}^{T} \\
                       &amp; = P.
    \end{align*}\]

<p>\(P\) étant une application linéaire de \(\mathbb{R}^{M} \rightarrow \mathbb{R}^{M}\), on a la somme directe</p>

\[\mathbb{R}^{M} = \ker P \oplus \mathrm{im} \, P,\]

<p>avec</p>

\[\mathrm{im}\,P = \ker(\mathbb{I}_{M}-P),\]

<p>or \(\ker(\mathbb{I}_{M}-P) = \ker(\mathbb{I}_{M}-(\mathbb{I}_{M} - \mathbf{e} \otimes \mathbf{e})) = \ker (\mathbf{e} \otimes \mathbf{e})\).</p>

\[\ker (\mathbf{e} \otimes \mathbf{e}) = \left\{ (x_{1}, \dots, x_{M}) \in \mathbb{R}^{M} \, : \, \sum_{i=1}^{M} x_{i} = 0 \right\}\]

<p>Cela définit bien un hyperplan, <strong>l’hyperplan des vecteurs centrés</strong>, ie de moyenne nulle. De plus, on a \(\mathbf{e}^{T}P = \mathbf{e}^{T}(\mathbb{I}_{M} - \mathbf{e} \otimes \mathbf{e}) = \mathbf{e}^{T} - (\mathbf{e}^{T} \cdot \mathbf{e}) \cdot \mathbf{e}^{T} = \mathbf{e}^{T} -  \mathbf{e}^{T} = 0\).</p>

<p><strong>Donc</strong> \(P\) <strong>est un opérateur de projection sur l’hyperplan orthogonal au vecteur unitaire</strong> \(\mathbf{e}^{T}\).</p>

<p>Notons alors par \(\left\langle \mathbb{1}_{M} \right\rangle\) la droite vectorielle engendrée par le vecteur de dimension \(M\) \(\mathbb{1}_{M} := (1, \dots, 1)\). On a alors la somme directe suivante.</p>

\[\mathbb{R}^{M} = \ker(\mathbf{e} \otimes \mathbf{e}) \oplus \left\langle \mathbb{1}_{M} \right\rangle\]

<hr />

<h2 id="application-à-la-descente-du-gradient">Application à la descente du gradient</h2>

<p>L’opérateur de projection orthogonale définie plus haut ne s’applique que sur des vecteurs colonnes. Pour l’appliquer au gradient de la fonction de perte associée à un réseau de neurones on l’étend par linéarité en un endomorphisme de l’espace vectoriel des matrices de tailles $M \times N$.</p>

\[\mathbf{P} \, : \, \mathbb{R}^{M\times N} \longrightarrow \mathbb{R}^{M\times N}\]

<p>Remarquez ici que le nombres de colonnes, bien que fixé à \(N\) n’a que peu d’importance</p>

\[\mathbb{R}^{M\times N} = \left( \ker(\mathbf{e} \otimes \mathbf{e}) \oplus \left\langle \mathbb{1}_{M} \right\rangle \right) ^{M \times N}\]

<p>On rappelle ici que la couche dense ou convolutive sur laquelle on opère <strong>est fixée</strong>.</p>

<p>Pour un réseau de neurones, <strong>on a donc un opérateur de centralisation \(\Phi_{CG}\), par couche dense et convolutive</strong>.</p>

<p>Pour chaque couche de matrice de poids \(W \in \mathbb{R}^{M \times N}\), on a donc :</p>

<ul>
  <li>un vecteur unitaire \(\mathbf{e}_{W} = \frac{1}{\sqrt{M}}\cdot \mathbb{1}_{M}\),</li>
  <li>et un opérateur de centralisation \(\Phi_{CG}\) projetant sur \(\ker (\mathbf{e}_{W} \otimes \mathbf{e}_{W})\) orthogonalement à \(\mathbf{e}_{W}^{T}\).</li>
</ul>

<p><img src="https://raw.githubusercontent.com/MathKlim/MathKlim.github.io/master/assets/img/geom_cg.svg?sanitize=true" alt="Alt text" /></p>

<p>Notons \(W^{t}\) la matrice des poids à l’itération \(t\) pour une couche fixée. Une équation de l’hyperplan sur lequel projette \(\Phi_{CG}\) est la suivante.</p>

\[\mathcal{H} := \left\{ -w \in \mathbb{R}^{M} \, : \, \mathbf{e}_{W}^{T} \cdot w  = 0 \right\}\]]]></content><author><name>Mathieu Klimczak</name></author><summary type="html"><![CDATA[La centralisation du gradient, nouvelle méthode d'optimisation de la SGD]]></summary></entry><entry><title type="html">Matrices, Tensors, and Machine Learning, a crash course (EN)</title><link href="https://mathklim.github.io/blog/2021/matrices-and-tensors/" rel="alternate" type="text/html" title="Matrices, Tensors, and Machine Learning, a crash course (EN)" /><published>2021-11-29T00:00:00+00:00</published><updated>2021-11-29T00:00:00+00:00</updated><id>https://mathklim.github.io/blog/2021/matrices-and-tensors</id><content type="html" xml:base="https://mathklim.github.io/blog/2021/matrices-and-tensors/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>TBA</p>

<p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine.
You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>.
If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p>

<p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph.
Here is an example:</p>

\[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\]

<p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p>

<hr />]]></content><author><name>Mathieu Klimczak</name></author><summary type="html"><![CDATA[What the hell is a tensor, and why do we care ?]]></summary></entry><entry><title type="html">a distill-style blog post</title><link href="https://mathklim.github.io/blog/2021/distill/" rel="alternate" type="text/html" title="a distill-style blog post" /><published>2021-05-22T00:00:00+00:00</published><updated>2021-05-22T00:00:00+00:00</updated><id>https://mathklim.github.io/blog/2021/distill</id><content type="html" xml:base="https://mathklim.github.io/blog/2021/distill/"><![CDATA[<p><strong>NOTE:</strong>
Citations, footnotes, and code blocks do not display correctly in the dark mode since distill does not support the dark mode by default.
If you are interested in correctly adding dark mode support for distill, please open <a href="https://github.com/alshedivat/al-folio/discussions">a discussion</a> and let us know.</p>

<h2 id="equations">Equations</h2>

<p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine.
You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>.
If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p>

<p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph.
Here is an example:</p>

\[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\]

<p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p>

<hr />

<h2 id="citations">Citations</h2>

<p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag.
The key attribute is a reference to the id provided in the bibliography.
The key attribute can take multiple ids, separated by commas.</p>

<p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover).
If you have an appendix, a bibliography is automatically created and populated in it.</p>

<p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover.
However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p>

<hr />

<h2 id="footnotes">Footnotes</h2>

<p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag.
The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p>

<hr />

<h2 id="code-blocks">Code Blocks</h2>

<p>Syntax highlighting is provided within <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> tags.
An example of inline code snippets: <code class="language-plaintext highlighter-rouge">&lt;d-code language="html"&gt;let x = 10;&lt;/d-code&gt;</code>.
For larger blocks of code, add a <code class="language-plaintext highlighter-rouge">block</code> attribute:</p>

<d-code block="" language="javascript">
  var x = 25;
  function(x) {
    return x * x;
  }
</d-code>

<p><strong>Note:</strong> <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> blocks do not look well in the dark mode.
You can always use the default code-highlight using the <code class="language-plaintext highlighter-rouge">highlight</code> liquid tag:</p>

<figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="kd">var</span> <span class="nx">x</span> <span class="o">=</span> <span class="mi">25</span><span class="p">;</span>
<span class="kd">function</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">return</span> <span class="nx">x</span> <span class="o">*</span> <span class="nx">x</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure>

<hr />

<h2 id="layouts">Layouts</h2>

<p>The main text column is referred to as the body.
It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p>

<div class="fake-img l-body">
  <p>.l-body</p>
</div>

<p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p>

<div class="fake-img l-page">
  <p>.l-page</p>
</div>

<p>All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:</p>

<div class="fake-img l-body-outset">
  <p>.l-body-outset</p>
</div>

<div class="fake-img l-page-outset">
  <p>.l-page-outset</p>
</div>

<p>Occasionally you’ll want to use the full browser width.
For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>.
You can also inset the element a little from the edge of the browser by using the inset variant.</p>

<div class="fake-img l-screen">
  <p>.l-screen</p>
</div>
<div class="fake-img l-screen-inset">
  <p>.l-screen-inset</p>
</div>

<p>The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code> sized text except on mobile screen sizes.</p>

<div class="fake-img l-gutter">
  <p>.l-gutter</p>
</div>

<hr />

<h2 id="other-typography">Other Typography</h2>

<p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p>

<p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p>

<p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p>

<p>Strikethrough uses two tildes. <del>Scratch this.</del></p>

<ol>
  <li>First ordered list item</li>
  <li>Another item
⋅⋅* Unordered sub-list.</li>
  <li>Actual numbers don’t matter, just that it’s a number
⋅⋅1. Ordered sub-list</li>
  <li>And another item.</li>
</ol>

<p>⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p>

<p>⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅
⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅
⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)</p>

<ul>
  <li>Unordered list can use asterisks</li>
  <li>Or minuses</li>
  <li>Or pluses</li>
</ul>

<p><a href="https://www.google.com">I’m an inline-style link</a></p>

<p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p>

<p><a href="https://www.mozilla.org">I’m a reference-style link</a></p>

<p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p>

<p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p>

<p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p>

<p>URLs and URLs in angle brackets will automatically get turned into links.
http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes
example.com (but not on Github, for example).</p>

<p>Some text to show that the reference links can follow later.</p>

<p>Here’s our logo (hover to see the title text):</p>

<p>Inline-style:
<img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1" /></p>

<p>Reference-style:
<img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2" /></p>

<p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nx">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="s">"Python syntax highlighting"</span>
<span class="k">print</span> <span class="n">s</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting.
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div>

<p>Colons can be used to align columns.</p>

<table>
  <thead>
    <tr>
      <th>Tables</th>
      <th style="text-align: center">Are</th>
      <th style="text-align: right">Cool</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>col 3 is</td>
      <td style="text-align: center">right-aligned</td>
      <td style="text-align: right">$1600</td>
    </tr>
    <tr>
      <td>col 2 is</td>
      <td style="text-align: center">centered</td>
      <td style="text-align: right">$12</td>
    </tr>
    <tr>
      <td>zebra stripes</td>
      <td style="text-align: center">are neat</td>
      <td style="text-align: right">$1</td>
    </tr>
  </tbody>
</table>

<p>There must be at least 3 dashes separating each header cell.
The outer pipes (|) are optional, and you don’t need to make the
raw Markdown line up prettily. You can also use inline Markdown.</p>

<table>
  <thead>
    <tr>
      <th>Markdown</th>
      <th>Less</th>
      <th>Pretty</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>Still</em></td>
      <td><code class="language-plaintext highlighter-rouge">renders</code></td>
      <td><strong>nicely</strong></td>
    </tr>
    <tr>
      <td>1</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>Blockquotes are very handy in email to emulate reply text.
This line is part of the same quote.</p>
</blockquote>

<p>Quote break.</p>

<blockquote>
  <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p>
</blockquote>

<p>Here’s a line for us to start with.</p>

<p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p>

<p>This line is also a separate paragraph, but…
This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[an example of a distill-style blog post and main elements]]></summary></entry></feed>