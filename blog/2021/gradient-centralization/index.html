<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Mathieu Klimczak


  | Gradient Centralization (FR)

</title>
<meta name="description" content="Mathieu's Website.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üî•</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="https://mathklim.github.io/blog/2021/gradient-centralization/">


<!-- Dark Mode -->
<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>


    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

    </style>
    
  </head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "Gradient Centralization (FR)",
      "description": "La centralisation du gradient, nouvelle m√©thode d'optimisation de la SGD",
      "published": "November 29, 2021",
      "authors": [
        
        {
          "author": "Mathieu Klimczak",
          "authorURL": "",
          "affiliations": [
            {
              "name": "CITC-EuraRFID",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://MathKlim.github.io/">
       Mathieu Klimczak
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item dropdown ">
              <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                submenus
                
              </a>
              <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
              
              
                <a class="dropdown-item" href="/publications/">publications</a>
              
              
              
                <div class="dropdown-divider"></div>
              
              
              
                <a class="dropdown-item" href="/projects/">projects</a>
              
              
              </div>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="post distill">

      <d-title>
        <h1>Gradient Centralization (FR)</h1>
        <p>La centralisation du gradient, nouvelle m√©thode d'optimisation de la SGD</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        
        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            
            <div><a href="#abstract">Abstract</a></div>
            
            
            <div><a href="#introduction">Introduction</a></div>
            
            
            <div><a href="#centralisation-du-gradient">Centralisation du gradient</a></div>
            
            
            <div><a href="#notations-et-conventions">Notations et conventions</a></div>
            
            
          </nav>
        </d-contents>
        

        <h2 id="abstract">Abstract</h2>

<p>Optimization techniques are of great importance to effectively and efficiently train a deep neural network (DNN). It has been shown that using the first and second order statistics (e.g., mean and variance) to perform Z-score standardization on network activations or weight vectors, such as batch normalization (BN) and weight standardization (WS), can improve the training performance. Different from these existing methods that mostly operate on activations or weights, we present a new optimization technique, namely gradient centralization (GC), which operates directly on gradients by centralizing the gradient vectors to have zero mean. GC can be viewed as a projected gradient descent method with a constrained loss function. We show that GC can regularize both the weight space and output feature space so that it can boost the generalization performance of DNNs. Moreover, GC improves the Lipschitzness of the loss function and its gradient so that the training process becomes more efficient and stable. GC is very simple to implement and can be easily embedded into existing gradient based DNN optimizers with only one line of code. It can also be directly used to fine-tune the pre-trained DNNs. Our experiments on various applications, including general image classification, fine-grained image classification, detection and segmentation, demonstrate that GC can consistently improve the performance of DNN learning. The code of GC can be found at <a href="https://github.com/Yonghongwei/Gradient-Centralization" target="_blank" rel="noopener noreferrer">this https URL</a>.</p>

<ul>
  <li><a href="https://arxiv.org/abs/2004.01461" target="_blank" rel="noopener noreferrer">ArXiv</a></li>
</ul>

<p>La centralisation du gradient, est une nouvelle m√©thode d‚Äôoptimisation des r√©seaux de neurones (plut√¥t une modification de celles d√©j√† existantes), qui se concentre sur le gradient lui m√™me afin de le centrer, i.e. √©tant donn√© le vecteur gradient \(\overrightarrow{\mathrm{grad}}\) obtenu en diff√©rentiant la fonction de perte \(\mathcal{L}_{\vartheta}\), on le centralise de fa√ßon √† obtenir \(\overrightarrow{\mathrm{grad}}_{CG}\) v√©rifiant l‚Äô√©glite suivante.</p>

\[\mathbb{E}(\overrightarrow{\mathrm{grad}}_{CG}) =0\]

<p>On le verra par la suite, cette m√©thode peut √™tre vu de fa√ßon g√©om√©trique comme une projection orthogonale du gradient sur un hyperplan contraignant la fonction de perte \(\mathcal{L}_{\vartheta}\).</p>

<h2 id="introduction">Introduction</h2>

<p>L‚Äô√©fficacit√© de l‚Äôentra√Ænement des r√©seaux de neurones d√©pend beaucoup des techniques d‚Äôoptimization employ√©es.</p>

<p>Le choix d‚Äôun optimiseur (Adam, RMSProp, Nesterov, etc) se fait g√©n√©ralement sur deux crit√®res:</p>

<ol>
  <li>L‚Äôacc√©l√©ration de l‚Äôentra√Ænement qui en r√©sulte.</li>
  <li>L‚Äôam√©lioration de la capacit√© de g√©n√©ralisation du r√©seau.</li>
</ol>

<p>D‚Äôautres techniques d‚Äôoptimisation existent :</p>
<ul>
  <li>Les m√©thodes d‚Äôinitialisation des poids (He, Xavier etc),</li>
  <li>Le choix des fonctions d‚Äôactivations (ReLU, mish),</li>
  <li>Le clipping du gradient,</li>
  <li>Mettre en place un taux d‚Äôapprentissage adaptatif.</li>
</ul>

<p>Il existe aussi des techniques de normalisation et de standardisation des poids, <strong>mais ces derni√®res ne sont pas applicables aux mod√®les pr√©-entrain√©s, dont les poids ne peuvent pas √™tre centr√©s, norm√©s</strong> (sinon on perd l‚Äôint√©r√™t du pr√©-entra√Ænement).</p>

<p>L‚Äôarticle apporte deux contributions :</p>

<ul>
  <li>Une nouvelle technique d‚Äôoptimisation bas√©e sur la centralisation du gradient, qui est facilement impl√©mentable, <a href="https://keras.io/examples/vision/gradient_centralization/#implement-gradient-centralization" target="_blank" rel="noopener noreferrer">voir le tutoriel sur keras.io</a>.</li>
  <li>Les aspects th√©oriques de cette centralisation du gradient. L‚Äôarticle montre en particuliers que la centralisation du gradient :
    <ol>
      <li>contraint la fonction de perte en ajoutant une nouvelle contrainte sur le vecteur de poids, ce qui permet de r√©gulariser l‚Äôespace des poids ainsi de celui des features de sortie.</li>
      <li>Am√©liore les propri√©t√©s lipschitzienne de la fonction de perte.</li>
    </ol>
  </li>
</ul>

<h3 id="d√©finition">D√©finition</h3>

<ol>
  <li>Une fonction \(f\) entre deux espaces m√©triques \(f \, : \, (X, d_{1}) \rightarrow (Y, d_{2})\) est lipschitz s‚Äôil existe une constante \(C &gt;0\) telle que :
\(\forall x, y \in X, d_{2}(f(x), f(y)) \leq C d_{1}(x,y).\)
En particulier, la fonction \(f\) est continue.</li>
  <li>Une fonction diff√©rentiable \(f\) entre deux espaces norm√©s \(f \, : \, (X, d_{1}) \rightarrow (Y, d_{2})\) est lipschitz lisse s‚Äôil existe une constante \(C &gt;0\) telle que :
\(\forall x, y \in X, \| \nabla f(x) - \nabla f(y) \| \leq C \| x - y \|.\)
(\(C\)-Lipschitz continuous gradient)</li>
  <li>La constante \(C\) est app√©l√©e la constante de Lipschitz de \(f\).</li>
</ol>

<h3 id="infos">Infos</h3>

<ul>
  <li><a href="https://math.stackexchange.com/questions/673898/lipschitz-smoothness-strong-convexity-and-the-hessian" target="_blank" rel="noopener noreferrer">Lipschitz Smoothness, Strong Convexity and the Hessian</a></li>
  <li><a href="https://xingyuzhou.org/blog/notes/Lipschitz-gradient" target="_blank" rel="noopener noreferrer">Lipschitz continuous gradient</a></li>
</ul>

<hr>

<h2 id="centralisation-du-gradient">Centralisation du gradient</h2>

<p>On sait que la <em>Batch Normalization</em> permet de r√©duire la constante de Lipschitz de la fonction de perte \(\mathcal{L}_{\vartheta}\) (TODO : Sources) et rend les gradients <em>plus lipschitz continus</em>.</p>

<p>Cependant, si la <em>batch size</em> est petite, la <em>Group Normalization</em> est plus appropri√©e, on perd alors les propri√©t√©s cit√©es ci-dessus.</p>

<p>Peut-on directement op√©rer sur le gradient pour stabiliser l‚Äôentra√Ænement ? Normaliser le gradient n‚Äôam√©liore pas la stabilit√© de l‚Äôentra√Ænement, on va dire le centrer, ie le rendre d‚Äôesp√©rance nulle.</p>

<hr>

<h2 id="notations-et-conventions">Notations et conventions</h2>

<ul>
  <li>Lorsque l‚Äôon parlera qu‚Äôune couche dense, on notera sa matrice de poids</li>
</ul>

\[W_{fc} \in \mathbb{R}^{C_{in} \times C_{out}}.\]

<ul>
  <li>Lorsque l‚Äôon parlera d‚Äôune couche convolutive, on notera sa matrice de poids</li>
</ul>

\[W_{conv} \in \mathbb{R}^{C_{in} \times C_{out} \times (k_{1}k_{2})},\]

<p>\(k_{1}\) et \(k_{2}\) √©tant les dimensions des noyaux de convolutions.</p>

<ul>
  <li>
    <p>Pour une couche quelconque, on utilisera la notation unifi√©e \(W \in \mathbb{R}^{M \times N}\). Pour \(W_{fc}\), \(M = C_{in}\), et pour \(W_{fc}\), \(M = C_{in}k_{1}k_{2}\).</p>
  </li>
  <li>
    <p>Pour une couche \(W \in \mathbb{R}^{M \times N}\), on notera \(w_{i} \in \mathbb{R}^{M}, i = 1, \dots, N\) sa \(i\)-i√®me colonne.</p>
  </li>
</ul>

<h3 id="convention-matricielle">Convention matricielle</h3>

<p>pour un vecteur d‚Äôentr√©e \(X\) de la couche \(W\), les features en sortie de la couche sont alors donn√©es par la formule suivante.</p>

\[out_{W}(X) := W^{T} \cdot X \qquad W^{T} \in \mathbb{R}^{C_{out} \times C_{in}}\]

<p>Ce qui veut dire que par convention, les vecteurs consid√©r√©s ici sont des <strong>vecteurs colonnes</strong>.</p>

\[out_{W}(X) =
\left[
\begin{array}{ccc}
    \rule[.5ex]{2.5ex}{0.5pt} &amp; w^{T}_{1} &amp; \rule[.5ex]{2.5ex}{0.5pt} \\
    \rule[.5ex]{2.5ex}{0.5pt} &amp; w^{T}_{2} &amp; \rule[.5ex]{2.5ex}{0.5pt}
\end{array}
\right]^{T}
\cdot
\begin{bmatrix}
        x_{1} \\
        x_{2}
\end{bmatrix}\]

\[W^{T} \cdot X =
\left[
\begin{array}{ccc}
    w_{1,1} &amp; w_{1,2} &amp; w_{1,3} \\
    w_{2,1} &amp; w_{2,2} &amp; w_{2,3}
\end{array}
\right]^{T}
\cdot
\begin{bmatrix}
        x_{1} \\
        x_{2}
\end{bmatrix}\]

<ul>
  <li>
    <p>Pour la fonction de perte \(\mathcal{L}_{\vartheta}\), on notera son gradient \(\nabla_{W}\mathcal{L}_{\vartheta}\) par rapport √† la matrice de poids \(W\), et \(\nabla_{w_{i}}\mathcal{L}_{\vartheta}\) son gradient par rapport au vecteur de poids \(w_{i}\).</p>
  </li>
  <li>
    <p>On note \(\mathbf{e} := \frac{1}{\sqrt{M}}\cdot \mathbb{1}_{M}\), vecteur unitaire de \(\mathbb{R}^{M}\).</p>
  </li>
</ul>

<hr>

<h2 id="formulation-de-la-centralisation-du-gradient">Formulation de la centralisation du gradient</h2>

<p>Pour une couche dense ou convolutive, supposons que l‚Äôon a obtenu le gradient via r√©tropropagation.</p>

<p>Pour un vecteur de poids \(w_{i}\), de gradient \(\nabla_{w_{i}}\mathcal{L}_{\vartheta}\), l‚Äôop√©rateur de centralisation du gradient, not√© \(\Phi_{CG}\), est alors d√©fini comme suit.</p>

\[\Phi_{CG}(\nabla_{w_{i}}\mathcal{L}_{\vartheta}) := \nabla_{w_{i}}\mathcal{L}_{\vartheta} - \mathbb{E}(\nabla_{w_{i}}\mathcal{L}_{\vartheta})\]

<p>O√π</p>

\[\mathbb{E}(\nabla_{w_{i}}\mathcal{L}_{\vartheta}) := \frac{1}{M}\sum_{j=1}^{M}\nabla_{w_{i,j}}\mathcal{L}_{\vartheta}.\]

<p>En d‚Äôautres termes, pour une matrice de poids, on calcule la moyenne de chaque vecteur colonne de cette matrice et on retire cette moyenne √† ses colonnes.</p>

<hr>

<h2 id="formulation-matricielle-et-repr√©sentation-g√©om√©trique">Formulation matricielle et repr√©sentation g√©om√©trique</h2>

<hr>

<h3 id="d√©finition--produit-de-kronecker">D√©finition : Produit de Kronecker</h3>

<p>Pour \(x\) et \(y\) deux vecteurs colonnes de dimensions \(M\), respectivement \(N\), le prodouit de Kronecker de \(x\) et \(y\), not√© \(x \cdot y^{T}\) ou \(x \otimes y\) est alors d√©fini de la fa√ßon suivante.</p>

\[x \cdot y^{T} := \left[ x_{i} \cdot y_{j}\right]\]

<p>O√π \(\left[ x_{i} \cdot y_{j}\right] \in \mathbb{R}^{M \times N}\) est la matrice dont les coefficients en les coordonn√©es \((i,j)\) sont donn√©es par le produit \(x_{i} \cdot y_{j}\).</p>

<p>Ce produit n‚Äôest pas commutatif. C‚Äôest un cas particulier de produit tensoriel, <a href="https://fr.wikipedia.org/wiki/Produit_matriciel#Produit_de_Kronecker" target="_blank" rel="noopener noreferrer">voir ici</a>.</p>

<hr>

<p>Dans le cas qui nous int√©resse ici, on applique le produit de Kronecker au vecteur suivant : \(\mathbf{e}\).</p>

\[\begin{align*}
\mathbf{e} \otimes \mathbf{e} &amp; := \frac{1}{\sqrt{M}}\cdot \mathbb{1} \otimes \frac{1}{\sqrt{M}}\cdot \mathbb{1} \\
                                &amp; = \frac{1}{M}\cdot \mathbb{1} \otimes \mathbb{1} \\
                                &amp; = \frac{1}{M}\cdot \mathbb{1} \otimes \mathbb{1} \\
                                &amp; = \frac{1}{M}\cdot\left[
                                    \begin{array}{ccc}
                                        1 &amp; \cdots &amp; 1 \\
                                        \vdots &amp;  &amp; \vdots \\
                                        1 &amp; \cdots &amp; 1
                                    \end{array}
                                    \right]
\end{align*}\]

<p>Ici, \(\mathbb{1} \otimes \mathbb{1}\) est donc une matrice carr√©e de taille \(M \times M\) o√π tous les coefficients sont √©gaux √† \(1\).</p>

<p>Remarquons que dans l‚Äôautre sens, le produit scalaire \(\mathbf{e}^{T} \cdot \mathbf{e}\) est √©gal √† \(1\), car \(\mathbf{e}\) est par d√©finition un vecteur unitaire.</p>

<p>Pour une matrice de poids \(W\), de gradient \(\nabla_{W}\mathcal{L}_{\vartheta}\), l‚Äôop√©rateur de centralisation du gradient, not√© \(\Phi_{CG}\), est alors d√©fini comme suit.</p>

\[\Phi_{CG}(\nabla_{W}\mathcal{L}_{\vartheta}) := P(\nabla_{W}\mathcal{L}_{\vartheta})\]

<p>O√π \(P := \mathbb{I}_{M} - \mathbf{e} \otimes \mathbf{e}\). \(\mathbb{I}_{M}\) √©tant la matrice identit√© de taille \(M\).</p>

<p>Avant de voir les propri√©t√©s de l‚Äôop√©rateur \(P\), v√©rifions que l‚Äôon obtient bien le m√™me r√©sultat avec cette d√©finition que la pr√©c√©dente.</p>

<p>On a \(\nabla_{W}\mathcal{L}_{\vartheta} := \left[ \nabla_{w_{1}}\mathcal{L}_{\vartheta} , \cdots, \nabla_{w_{N}}\mathcal{L}_{\vartheta} \right]\), d‚Äôo√π</p>

\[\begin{align*}
    \Phi_{CG}(\nabla_{W}\mathcal{L}_{\vartheta}) &amp; = \left(\mathbb{I}_{M} - \mathbf{e} \otimes \mathbf{e} \right)\cdot \left[ \nabla_{w_{1}}\mathcal{L}_{\vartheta} , \cdots, \nabla_{w_{N}}\mathcal{L}_{\vartheta} \right] \\
                                                 &amp; = \left[ \nabla_{w_{1}}\mathcal{L}_{\vartheta} , \cdots, \nabla_{w_{N}}\mathcal{L}_{\vartheta} \right] - \frac{1}{M}\cdot \mathbb{1} \cdot \mathbb{1}^{T} \cdot \left[ \nabla_{w_{1}}\mathcal{L}_{\vartheta} , \cdots, \nabla_{w_{N}}\mathcal{L}_{\vartheta} \right].
    \end{align*}\]

<p>Par lin√©arit√©, il suffit de le v√©rifier sur chaque colonne.</p>

\[\begin{align*}
    \nabla_{w_{i}}\mathcal{L}_{\vartheta} - \frac{1}{M}\cdot \mathbb{1} \cdot \mathbb{1}^{T} \cdot \nabla_{w_{i}}\mathcal{L}_{\vartheta} &amp; = \nabla_{w_{i}}\mathcal{L}_{\vartheta} - \frac{1}{M}\cdot \mathbb{1} \cdot \left( \mathbb{1}^{T} \cdot \nabla_{w_{i}}\mathcal{L}_{\vartheta} \right) \\
    &amp; = \nabla_{w_{i}}\mathcal{L}_{\vartheta} - \frac{1}{M}\cdot \mathbb{1} \cdot \left(\sum_{j=1}^{M}\nabla_{w_{i,j}}\mathcal{L}_{\vartheta}\right) \\
    &amp; = \nabla_{w_{i}}\mathcal{L}_{\vartheta} - \mathbb{E}(\nabla_{w_{i}}\mathcal{L}_{\vartheta})
    \end{align*}\]

<p>On a donc bien le m√™me r√©sultat, peu importe la d√©finition. Passons donc maintenant aux propri√©t√©es de l‚Äôop√©rateur \(P\).</p>

<h3 id="th√©or√®me">Th√©or√®me</h3>

<p>L‚Äôop√©rateur \(P\) est idempotent et d√©finit une projection sur l‚Äôhyperplan orthogonal au vecteur unitaire \(\mathbf{e}^{T}\).</p>

<h4 id="preuve">Preuve</h4>

<p>Montrons premi√®rement que c‚Äôest un op√©rateur idempotent. Pour cela il suffit de montrer que \(P^{2} = P = P^{T}\).</p>

<p>Tout d‚Äôabord, on a \(P^{T} = (\mathbb{I}_{M} - \mathbf{e} \otimes \mathbf{e})^{T} = \mathbb{I}_{M}^{T} - (\mathbf{e} \cdot \mathbf{e}^{T})^{T} = \mathbb{I}_{M} - \mathbf{e} \cdot \mathbf{e}^{T} = P\).</p>

<p>D‚Äôo√π,</p>

\[\begin{align*}
        P^{2} = P^{T}P &amp; = (\mathbb{I}_{M} - \mathbf{e} \otimes \mathbf{e})^{T} \cdot (\mathbb{I}_{M} - \mathbf{e} \otimes \mathbf{e}) \\
                       &amp; = \mathbb{I}_{M} - 2 \mathbf{e}\cdot \mathbf{e}^{T} + \mathbf{e}\cdot (\mathbf{e}^{T}\cdot \mathbf{e})\cdot \mathbf{e}^{T} \\
                       &amp; = \mathbb{I}_{M} - 2 \mathbf{e}\cdot \mathbf{e}^{T} + \mathbf{e}\cdot\mathbf{e}^{T} \\
                       &amp; = \mathbb{I}_{M} - \mathbf{e}\cdot \mathbf{e}^{T} \\
                       &amp; = P.
    \end{align*}\]

<p>\(P\) √©tant une application lin√©aire de \(\mathbb{R}^{M} \rightarrow \mathbb{R}^{M}\), on a la somme directe</p>

\[\mathbb{R}^{M} = \ker P \oplus \mathrm{im} \, P,\]

<p>avec</p>

\[\mathrm{im}\,P = \ker(\mathbb{I}_{M}-P),\]

<p>or \(\ker(\mathbb{I}_{M}-P) = \ker(\mathbb{I}_{M}-(\mathbb{I}_{M} - \mathbf{e} \otimes \mathbf{e})) = \ker (\mathbf{e} \otimes \mathbf{e})\).</p>

\[\ker (\mathbf{e} \otimes \mathbf{e}) = \left\{ (x_{1}, \dots, x_{M}) \in \mathbb{R}^{M} \, : \, \sum_{i=1}^{M} x_{i} = 0 \right\}\]

<p>Cela d√©finit bien un hyperplan, <strong>l‚Äôhyperplan des vecteurs centr√©s</strong>, ie de moyenne nulle. De plus, on a \(\mathbf{e}^{T}P = \mathbf{e}^{T}(\mathbb{I}_{M} - \mathbf{e} \otimes \mathbf{e}) = \mathbf{e}^{T} - (\mathbf{e}^{T} \cdot \mathbf{e}) \cdot \mathbf{e}^{T} = \mathbf{e}^{T} -  \mathbf{e}^{T} = 0\).</p>

<p><strong>Donc</strong> \(P\) <strong>est un op√©rateur de projection sur l‚Äôhyperplan orthogonal au vecteur unitaire</strong> \(\mathbf{e}^{T}\).</p>

<p>Notons alors par \(\left\langle \mathbb{1}_{M} \right\rangle\) la droite vectorielle engendr√©e par le vecteur de dimension \(M\) \(\mathbb{1}_{M} := (1, \dots, 1)\). On a alors la somme directe suivante.</p>

\[\mathbb{R}^{M} = \ker(\mathbf{e} \otimes \mathbf{e}) \oplus \left\langle \mathbb{1}_{M} \right\rangle\]

<hr>

<h2 id="application-√†-la-descente-du-gradient">Application √† la descente du gradient</h2>

<p>L‚Äôop√©rateur de projection orthogonale d√©finie plus haut ne s‚Äôapplique que sur des vecteurs colonnes. Pour l‚Äôappliquer au gradient de la fonction de perte associ√©e √† un r√©seau de neurones on l‚Äô√©tend par lin√©arit√© en un endomorphisme de l‚Äôespace vectoriel des matrices de tailles $M \times N$.</p>

\[\mathbf{P} \, : \, \mathbb{R}^{M\times N} \longrightarrow \mathbb{R}^{M\times N}\]

<p>Remarquez ici que le nombres de colonnes, bien que fix√© √† \(N\) n‚Äôa que peu d‚Äôimportance</p>

\[\mathbb{R}^{M\times N} = \left( \ker(\mathbf{e} \otimes \mathbf{e}) \oplus \left\langle \mathbb{1}_{M} \right\rangle \right) ^{M \times N}\]

<p>On rappelle ici que la couche dense ou convolutive sur laquelle on op√®re <strong>est fix√©e</strong>.</p>

<p>Pour un r√©seau de neurones, <strong>on a donc un op√©rateur de centralisation \(\Phi_{CG}\), par couche dense et convolutive</strong>.</p>

<p>Pour chaque couche de matrice de poids \(W \in \mathbb{R}^{M \times N}\), on a donc :</p>

<ul>
  <li>un vecteur unitaire \(\mathbf{e}_{W} = \frac{1}{\sqrt{M}}\cdot \mathbb{1}_{M}\),</li>
  <li>et un op√©rateur de centralisation \(\Phi_{CG}\) projetant sur \(\ker (\mathbf{e}_{W} \otimes \mathbf{e}_{W})\) orthogonalement √† \(\mathbf{e}_{W}^{T}\).</li>
</ul>

<p><img src="https://raw.githubusercontent.com/MathKlim/MathKlim.github.io/master/assets/img/geom_cg.svg?sanitize=true" alt="Alt text"></p>

<p>Notons \(W^{t}\) la matrice des poids √† l‚Äôit√©ration \(t\) pour une couche fix√©e. Une √©quation de l‚Äôhyperplan sur lequel projette \(\Phi_{CG}\) est la suivante.</p>

\[\mathcal{H} := \left\{ -w \in \mathbb{R}^{M} \, : \, \mathbf{e}_{W}^{T} \cdot w  = 0 \right\}\]


      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    ¬© Copyright 2022 Mathieu  Klimczak.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <d-bibliography src="/assets/bibliography/">
  </d-bibliography>

</html>
